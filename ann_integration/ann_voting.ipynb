{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64876c23",
   "metadata": {},
   "source": [
    "# Annotation Voting\n",
    "\n",
    "Each method used to predict cell type annotations is considered as part of the votation in this way:\n",
    "\n",
    "Let $w$ be the weight for each method in $M$ and $k$ a cell type annotation in $K$\n",
    "\n",
    "$$P(k) = \\sum_{i=1}^M w_i * P(k|m_i) $$\n",
    "\n",
    "\n",
    "Each annotation method $m_i \\in M$ provides either a predicted label $k_i \\in K \\cup \\{unknown\\}$, and an associated confidence score $c_i \\in [0,1]$\n",
    "\n",
    "We define a normalized weight $w_i$ for each method, reflecting its global concordance.\n",
    "\n",
    "Then, for a given cell, the ensemble assigns a probability to each possible cell type $k \\in K$ as:\n",
    "\n",
    "$$P(k)=\\frac{1}{Z} \\sum_{i=1}^M w_i P(k∣mi)$$\n",
    "\n",
    "where $Z=\\sum_{i=1}^M w_i$ is a normalization constant ensuring $\\sum_k P(k)=1$.\n",
    "\n",
    "\n",
    "## Final ensemble decision\n",
    "\n",
    "The ensemble’s predicted label is:\n",
    "\n",
    "$$\\hat{k} = \\text{arg max}_{k \\in K}(P(k))$$\n",
    "\n",
    "and uncertainty (entropy) can be quantified as:\n",
    "\n",
    "$$H = -\\sum_{k \\in K} P(k) \\log{P(k)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a7b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_predictions = \"../data/ann_integration/predictions.csv\"\n",
    "path_to_adata = \"../data/ann_integration/adata.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc08738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scanpy as sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d885de9",
   "metadata": {},
   "source": [
    "## Metrics for Predictions\n",
    "\n",
    "#### - Fleiss' Kappa\n",
    "Measures the degree of agreement between various categorical predictors (comparing it to a random choice)\n",
    "\n",
    ">Let ( N ) be the number of cells, ( M ) the number of models, and ( K ) the number of possible labels.\n",
    "For each cell ( j ), let ( n_{jk} ) be the number of models that assigned label ( k ).\n",
    "Then the agreement for that cell is:\n",
    ">$$P_j = \\frac{1}{M(M-1)} \\sum_{k=1}^{K} n_{jk}(n_{jk} - 1)$$\n",
    ">and the mean agreement across all cells:\n",
    ">$$\\bar{P} = \\frac{1}{N} \\sum_{j=1}^{N} P_j$$\n",
    ">Let the expected agreement by chance be:\n",
    ">$$\\bar{P}*e = \\sum*{k=1}^{K} p_k^2, \\quad \\text{where} \\quad p_k = \\frac{1}{N M} \\sum_{j=1}^{N} n_{jk}$$\n",
    ">Then Fleiss’ Kappa is given by:\n",
    ">$$\\kappa = \\frac{\\bar{P} - \\bar{P}_e}{1 - \\bar{P}_e}$$\n",
    ">A value of $\\kappa = 1$ indicates perfect agreement, $\\kappa = 0$ corresponds to random labeling, and negative values indicate systematic disagreement.\n",
    "\n",
    "\n",
    "#### - Pairwise Agreement\n",
    "\n",
    "Quantifies the proportion of cells for which two models give the same label, averaged over all model pairs.\n",
    "\n",
    ">For models $m_a, m_b \\in M$:\n",
    ">$$A_{ab} = \\frac{1}{N} \\sum_{j=1}^{N} [k_{a,j} = k_{b,j}]$$\n",
    ">The overall pairwise agreement is the mean over all unique model pairs:\n",
    ">$$\\bar{A} = \\frac{2}{M(M-1)} \\sum_{a < b} A_{ab}$$\n",
    ">This metric can be computed globally, per cluster, or per label to identify systematic disagreements between models.\n",
    "\n",
    "\n",
    "#### - Ensemble Entropy\n",
    "\n",
    "Measures the uncertainty of the ensemble prediction for each cell, given the distribution of predicted labels across models.\n",
    "\n",
    ">For a cell $j$, let $P_j(k)$ be the ensemble probability of label $k$, defined as:\n",
    ">$$P_j(k) = \\frac{1}{Z_j} \\sum_{i=1}^{M} w_i , c_{ij} , [k_{ij} = k]$$\n",
    ">where $w_i$ is the model’s weight, $c_{ij}$ its confidence score for that cell, and $Z_j$ the normalization constant.\n",
    ">Then the entropy for cell $j$ is:\n",
    ">$$H_j = - \\sum_{k=1}^{K} P_j(k) , \\log P_j(k)$$\n",
    ">Averaging across all cells gives the global ensemble entropy:\n",
    ">$$\\bar{H} = \\frac{1}{N} \\sum_{j=1}^{N} H_j$$\n",
    ">Low entropy indicates consistent predictions (high agreement), while high entropy reflects uncertainty or label conflict among models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300a348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11216c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _compute_fleiss_kappa(preds):\n",
    "    \"\"\"Compute Fleiss' Kappa for a set of categorical predictions.\"\"\"\n",
    "    preds = preds.dropna(how=\"all\")\n",
    "    if preds.empty or preds.shape[1] < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Map all labels to integers\n",
    "    cats = pd.Categorical(pd.concat([preds[c] for c in preds], axis=0).dropna()).categories\n",
    "    label_to_int = {cat: i for i, cat in enumerate(cats)}\n",
    "    ratings = preds.applymap(lambda x: label_to_int.get(x, np.nan)).dropna()\n",
    "    \n",
    "    n_items = len(ratings)\n",
    "    n_models = ratings.shape[1]\n",
    "    n_cats = len(cats)\n",
    "    \n",
    "    # Build category count matrix\n",
    "    counts = np.zeros((n_items, n_cats), dtype=int)\n",
    "    for j in range(n_models):\n",
    "        for i, val in enumerate(ratings.iloc[:, j]):\n",
    "            if not np.isnan(val):\n",
    "                counts[i, int(val)] += 1\n",
    "    try:\n",
    "        return fleiss_kappa(counts)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _pairwise_agreement(preds):\n",
    "    \"\"\"Compute mean ARI and NMI across all pairs.\"\"\"\n",
    "    preds = preds.dropna(how=\"all\")\n",
    "    if preds.shape[1] < 2:\n",
    "        return np.nan, np.nan\n",
    "    ari, nmi = [], []\n",
    "    for c1, c2 in combinations(preds.columns, 2):\n",
    "        common = preds[[c1, c2]].dropna()\n",
    "        if len(common) > 5:\n",
    "            ari.append(adjusted_rand_score(common[c1], common[c2]))\n",
    "            nmi.append(normalized_mutual_info_score(common[c1], common[c2]))\n",
    "    return np.nanmean(ari) if ari else np.nan, np.nanmean(nmi) if nmi else np.nan\n",
    "\n",
    "\n",
    "def _ensemble_entropy(preds):\n",
    "    \"\"\"Compute per-cell entropy of model predictions.\"\"\"\n",
    "    valid = preds.dropna(axis=1, how='all')\n",
    "    cats = pd.Categorical(pd.concat([valid[c] for c in valid], axis=0).dropna()).categories\n",
    "    n_cats = len(cats)\n",
    "    label_to_int = {cat: i for i, cat in enumerate(cats)}\n",
    "    \n",
    "    counts = np.zeros((len(valid), n_cats))\n",
    "    for j in range(valid.shape[1]):\n",
    "        for i, val in enumerate(valid.iloc[:, j]):\n",
    "            if val in label_to_int:\n",
    "                counts[i, label_to_int[val]] += 1\n",
    "    probs = counts / np.maximum(counts.sum(axis=1, keepdims=True), 1)\n",
    "    ent = entropy(probs.T)\n",
    "    consensus = np.max(probs, axis=1)\n",
    "    return ent, consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6991f24",
   "metadata": {},
   "source": [
    "### Model's Weight Calculation\n",
    "\n",
    "Define Model's Weight for score by its average mean Predicting Score and average agreement. \n",
    "\n",
    "$$w_m = \\frac{\\text{mean(PredScore)}_m \\times \\text{mean(agreement)}_m}{\\sum w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6e063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_model_weights(df, model_names):\n",
    "    \"\"\"Compute per-model weights based on confidence and mutual agreement.\"\"\"\n",
    "    # Mean confidence\n",
    "    confs = {\n",
    "        m: df[f\"{m}_pred_score\"].mean(skipna=True)\n",
    "        for m in model_names if f\"{m}_pred_score\" in df.columns\n",
    "    }\n",
    "    # Agreement: average ARI vs others\n",
    "    aris = {}\n",
    "    for m1 in model_names:\n",
    "        others = [m2 for m2 in model_names if m2 != m1 and f\"{m2}_pred\" in df.columns]\n",
    "        ari_vals = []\n",
    "        for m2 in others:\n",
    "            c1, c2 = f\"{m1}_pred\", f\"{m2}_pred\"\n",
    "            common = df[[c1, c2]].dropna()\n",
    "            if len(common) > 5:\n",
    "                ari_vals.append(adjusted_rand_score(common[c1], common[c2]))\n",
    "        aris[m1] = np.nanmean(ari_vals) if ari_vals else np.nan\n",
    "    # Combine\n",
    "    weights = {}\n",
    "    for m in model_names:\n",
    "        w = (confs.get(m, 1) * aris.get(m, 1))\n",
    "        weights[m] = w if not np.isnan(w) else 1\n",
    "    # Normalize\n",
    "    total = np.sum(list(weights.values()))\n",
    "    return {m: w / total if total > 0 else 1/len(weights) for m, w in weights.items()}\n",
    "\n",
    "\n",
    "def _weighted_majority_vote(df, model_names, weights):\n",
    "    \"\"\"Compute weighted ensemble label based on model confidence and agreement.\"\"\"\n",
    "    preds = df[[f\"{m}_pred\" for m in model_names if f\"{m}_pred\" in df.columns]]\n",
    "    all_labels = pd.unique(preds.values.ravel()[~pd.isnull(preds.values.ravel())])\n",
    "    weighted_labels = []\n",
    "    for i, row in preds.iterrows():\n",
    "        label_weights = {lab: 0 for lab in all_labels}\n",
    "        for m in model_names:\n",
    "            val = row.get(f\"{m}_pred\", np.nan)\n",
    "            if pd.notna(val):\n",
    "                label_weights[val] += weights.get(m, 1)\n",
    "        # Pick label with max total weight\n",
    "        weighted_labels.append(max(label_weights, key=label_weights.get))\n",
    "    return pd.Series(weighted_labels, index=df.index, name=\"ensemble_label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ef03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_ensemble(df, embedding=None, cluster_col=\"cluster\"):\n",
    "    \"\"\"\n",
    "    Evaluate ensemble model consistency for scRNA-seq cross-annotation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain '{model}_pred' columns (and optionally '{model}_pred_score').\n",
    "        Must also contain a cluster column ('cluster' or specified cluster_col).\n",
    "    embedding : np.ndarray, optional\n",
    "        PCA or Harmony embedding (n_cells x n_dims) for silhouette evaluation.\n",
    "    cluster_col : str, default 'cluster'\n",
    "        Column name defining cell clusters (e.g., 'leiden').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict with:\n",
    "      - global_metrics : pd.Series\n",
    "      - model_weights : pd.Series\n",
    "      - per_cell : pd.DataFrame\n",
    "      - per_cluster : pd.DataFrame\n",
    "      - per_label : pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    model_names = sorted({c.split('_pred')[0] for c in df.columns if c.endswith('_pred')})\n",
    "    pred_cols = [f\"{m}_pred\" for m in model_names if f\"{m}_pred\" in df.columns]\n",
    "    preds = df[pred_cols]\n",
    "    clusters = df[cluster_col]\n",
    "    \n",
    "    # --- COMPUTE MODEL WEIGHTS ---\n",
    "    weights = _compute_model_weights(df, model_names)\n",
    "    \n",
    "    # --- WEIGHTED ENSEMBLE LABEL ---\n",
    "    df[\"ensemble_label\"] = _weighted_majority_vote(df, model_names, weights)\n",
    "    \n",
    "    # --- GLOBAL METRICS ---\n",
    "    ent, consensus = _ensemble_entropy(preds)\n",
    "    ari, nmi = _pairwise_agreement(preds)\n",
    "    global_metrics = pd.Series({\n",
    "        \"fleiss_kappa\": _compute_fleiss_kappa(preds),\n",
    "        \"mean_ARI\": ari,\n",
    "        \"mean_NMI\": nmi,\n",
    "        \"mean_entropy\": np.nanmean(ent),\n",
    "        \"entropy_std\": np.nanstd(ent),\n",
    "        \"mean_consensus\": np.nanmean(consensus)\n",
    "    })\n",
    "    if embedding is not None:\n",
    "        labels_int = LabelEncoder().fit_transform(df[\"ensemble_label\"])\n",
    "        try:\n",
    "            global_metrics[\"silhouette_score\"] = silhouette_score(embedding, labels_int)\n",
    "        except Exception:\n",
    "            global_metrics[\"silhouette_score\"] = np.nan\n",
    "    \n",
    "    # --- PER CELL ---\n",
    "    per_cell = pd.DataFrame({\n",
    "        \"entropy\": ent,\n",
    "        \"consensus\": consensus,\n",
    "        cluster_col: clusters,\n",
    "        \"ensemble_label\": df[\"ensemble_label\"]\n",
    "    }, index=df.index)\n",
    "    \n",
    "    # --- PER CLUSTER ---\n",
    "    cluster_results = []\n",
    "    for cluster_id, subset in df.groupby(cluster_col):\n",
    "        preds_sub = subset[pred_cols]\n",
    "        ent_sub, cons_sub = _ensemble_entropy(preds_sub)\n",
    "        ari_sub, nmi_sub = _pairwise_agreement(preds_sub)\n",
    "        cluster_results.append({\n",
    "            cluster_col: cluster_id,\n",
    "            \"fleiss_kappa\": _compute_fleiss_kappa(preds_sub),\n",
    "            \"mean_ARI\": ari_sub,\n",
    "            \"mean_NMI\": nmi_sub,\n",
    "            \"mean_entropy\": np.nanmean(ent_sub),\n",
    "            \"entropy_std\": np.nanstd(ent_sub),\n",
    "            \"mean_consensus\": np.nanmean(cons_sub)\n",
    "        })\n",
    "    per_cluster = pd.DataFrame(cluster_results)\n",
    "    \n",
    "    # --- PER ENSEMBLE LABEL ---\n",
    "    label_results = []\n",
    "    for label, subset in df.groupby(\"ensemble_label\"):\n",
    "        preds_sub = subset[pred_cols]\n",
    "        ent_sub, cons_sub = _ensemble_entropy(preds_sub)\n",
    "        ari_sub, nmi_sub = _pairwise_agreement(preds_sub)\n",
    "        label_results.append({\n",
    "            \"ensemble_label\": label,\n",
    "            \"fleiss_kappa\": _compute_fleiss_kappa(preds_sub),\n",
    "            \"mean_ARI\": ari_sub,\n",
    "            \"mean_NMI\": nmi_sub,\n",
    "            \"mean_entropy\": np.nanmean(ent_sub),\n",
    "            \"entropy_std\": np.nanstd(ent_sub),\n",
    "            \"mean_consensus\": np.nanmean(cons_sub),\n",
    "            \"n_cells\": len(subset)\n",
    "        })\n",
    "    per_label = pd.DataFrame(label_results)\n",
    "    \n",
    "    return {\n",
    "        \"global_metrics\": global_metrics,\n",
    "        \"model_weights\": pd.Series(weights),\n",
    "        \"per_cell\": per_cell,\n",
    "        \"per_cluster\": per_cluster,\n",
    "        \"per_label\": per_label\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2be5e",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86cf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_to_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2428c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read(path_to_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d46f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs in predictions with 'unknown'\n",
    "for col in df.columns:\n",
    "    if col.endswith('_pred'):\n",
    "        df[col] = df[col].fillna('unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assuming df has:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ['scanvi_pred', 'scanvi_pred_score', 'celltypist_pred', 'celltypist_pred_score', 'cluster']\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = evaluate_ensemble(\u001b[43mdf\u001b[49m, embedding=adata.obsm[\u001b[33m'\u001b[39m\u001b[33mX_pca\u001b[39m\u001b[33m'\u001b[39m], label_col=\u001b[33m'\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGlobal metrics:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, results[\u001b[33m'\u001b[39m\u001b[33mglobal_metrics\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mper_cluster\u001b[39m\u001b[33m'\u001b[39m].to_csv(\u001b[33m\"\u001b[39m\u001b[33mensemble_per_cluster.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming df has:\n",
    "# ['scanvi_pred', 'scanvi_pred_score', 'celltypist_pred', 'celltypist_pred_score', 'cluster']\n",
    "\n",
    "results = evaluate_ensemble(df, embedding=adata.obsm['X_pca'], label_col='cluster')\n",
    "\n",
    "print(\"Global metrics:\\n\", results['global_metrics'])\n",
    "results['per_cluster'].to_csv(\"ensemble_per_cluster.csv\", index=False)\n",
    "results['per_label'].to_csv(\"ensemble_per_label.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95efb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fd84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce70bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf064d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
